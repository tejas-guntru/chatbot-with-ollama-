# Local AI Chat Interface

A beautiful, self-hosted AI chatbot that runs completely locally using Ollama and Python Flask. Features a modern dark interface, multiple model support, and GPU acceleration - no API costs or internet required.

![AI Chat Interface](https://img.shields.io/badge/AI-Local%20Only-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Ollama](https://img.shields.io/badge/Ollama-Required-orange)

## ‚ú® Features

- **100% Local** - No API costs, no data leaves your machine
- **GPU Accelerated** - Uses your NVIDIA/AMD GPU for fast inference
- **Multiple Models** - Support for Llama, Mistral, Gemma, Phi-3 and more
- **Beautiful UI** - Modern dark theme with smooth animations
- **Real-time Chat** - Typing indicators and instant responses
- **Private & Secure** - Complete conversation privacy

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- [Ollama](https://ollama.com) installed
- NVIDIA/AMD GPU (recommended)

### Installation

1. **Install Ollama**:
   ```bash
   # Windows
   winget install Ollama.Ollama
   
   # Mac/Linux
   curl -fsSL https://ollama.com/install.sh | sh
ü§ù Contributing
Contributions welcome! Feel free to submit issues and enhancement requests.\
